# -*- coding: utf-8 -*-
"""Project Sprint 13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13QmPy9UVODVQdHRgwlbXvw4PKAdGLDMs

# **Forecasting Project**

## **Introduction**

### **Goal**
The goal of this project is to develop a predictive model to forecast the number of taxi orders for the upcoming hour. This model will assist Sweet Lift, a taxi company, in attracting more drivers during peak hours and optimizing operational efficiency. The target is to achieve a root mean square error (RMSE) of 48 or less on the test set.

### **Steps**

1. **Data Acquisition and Resampling:**
   - Download the dataset from the provided file path.
   - Resample the data to aggregate it on an hourly basis.

2. **Data Analysis:**
   - Perform a comprehensive analysis of the resampled data to identify patterns and trends.

3. **Model Training and Evaluation:**
   - Train various models with different hyperparameters to determine the most effective forecasting approach.
   - Ensure the test sample represents 10% of the original dataset.
   - Evaluate the model's performance using the test sample, ensuring the RMSE does not exceed 48.

### **Data Description**
The dataset includes historical taxi order data with the number of orders recorded in the `num_orders` column. This data will be utilized for resampling, analysis, and model training to predict future taxi order volumes.

## **Data Loadment and Exploratory**
"""

!pip install catboost

# Import necessary library

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

"""### **Data Loadment**"""

# Data Loadment

path = '/content/taxi.csv'
df = pd.read_csv(path, index_col=[0], parse_dates=[0])
df.head(5)

# Data shape

df.shape

df.info()

# missing value checking

df.isnull().sum()

# duplicated data checking

df.duplicated().sum()

"""**Insight**

1. Data provided contains 26496 rows and 1 column of number of order for each 10 minutes of time interval.
2. There are no missing value has been found.
3. There are no duplicated value has been found.

### **Data Exploratory**
"""

df = df.resample('1H').sum()

df_eda = df.copy()
df_eda.tail(5)

df_eda_daily = df_eda.resample('1D').sum()
df_eda_daily.plot()

df_eda_hourly = df_eda.resample('1H').sum()
df_eda_hourly.plot()

df_eda.sort_index(inplace=True)
df_eda = df_eda['2018-06':'2018-08'].resample('1D').sum()
df_eda['rolling_mean'] = df_eda.rolling(10).mean()
df_eda.plot()

decomposed = seasonal_decompose(df)
plt.figure(figsize=(6, 8))

plt.subplot(311)
decomposed.trend.plot(ax=plt.gca())
plt.title('Trend')

plt.subplot(312)
decomposed.seasonal.plot(ax=plt.gca())
plt.title('Seasonality')

plt.subplot(313)
decomposed.resid.plot(ax=plt.gca())
plt.title('Residuals')

plt.tight_layout()
plt.show()

"""**Insigth**

The data shows consistency of increase order started from March to August 2018.

## **Machine Learning Development**

Create a function to add time-based features and lagged values to a DataFrame for use in time series forecasting.
"""

def make_features(df, max_lag=4, rolling_mean_size=10):
    df['month'] = df.index.month
    df['day'] = df.index.day
    df['dayofweek_num'] = df.index.dayofweek
    df['hour'] = df.index.hour

    for lag in range(1, max_lag + 1):
        df['lag_{}'.format(lag)] = df['num_orders'].shift(lag)

    df['rolling_mean'] = df['num_orders'].shift().rolling(rolling_mean_size).mean()

make_features(df)
df = df.dropna()
print(df.shape)

df.head(2)

# Splitting the data into train, test, and valid dataset

train_valid, test = train_test_split(df, shuffle=False, test_size=0.1)
train, valid = train_test_split(train_valid, shuffle=False, test_size=0.1)

print(train.index.min(), train.index.max())
print(train.shape)
print(test.index.min(), test.index.max())
print(test.shape)
print(valid.index.min(), valid.index.max())
print(valid.shape)

X_train = train.drop(['num_orders'], axis=1)
y_train = train['num_orders']
X_test = test.drop(['num_orders'], axis=1)
y_test = test['num_orders']
X_valid = valid.drop(['num_orders'], axis=1)
y_valid = valid['num_orders']

X_train.shape, y_train.shape

X_test.shape, y_test.shape

X_valid.shape, y_valid.shape

def rmse(true, pred):
    return mean_squared_error(true, pred)**0.5

print('Mean value:', test['num_orders'].mean())
pred_previous = test.shift()
pred_previous.iloc[0] = train.iloc[-1]

print('RMSE prev:', rmse(test['num_orders'], pred_previous['num_orders']))
pred_mean = np.ones(test['num_orders'].shape) * train['num_orders'].mean()
print('RMSE mean:', rmse(test['num_orders'], pred_mean))

"""### **Linear Regression**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Linear Regression Model
# 
# lr = LinearRegression()
# lr.fit(X_train, y_train)
# 
# pred_train = lr.predict(X_train)
# pred_valid = lr.predict(X_valid)
# 
# print('RMSE train:', rmse(y_train, pred_train).round(3))
# print('RMSE valid:', rmse(y_valid, pred_valid).round(3))
# print('')

"""### **Random Forest Regressor**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Random Forest Regressor Model
# 
# depth = [2, 4, 6, 8, None]
# 
# for max_depth in depth:
#     rf = RandomForestRegressor(n_estimators=500, max_depth=max_depth)
#     rf.fit(X_train, y_train)
# 
#     pred_train = rf.predict(X_train)
#     pred_valid = rf.predict(X_valid)
# 
#     print('Max depth:', max_depth)
#     print('RMSE train:', rmse(y_train, pred_train).round(3))
#     print('RMSE valid:', rmse(y_valid, pred_valid).round(3))
#     print('')

"""### **Light GBM**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Light GBM Model
# 
# lg = LGBMRegressor(learning_rate=0.02, num_iterations=10, objective='rmse')
# lg.fit(X_train, y_train, eval_set=(X_valid, y_valid))
# 
# pred_train = lg.predict(X_train)
# pred_valid = lg.predict(X_valid)
# 
# print('')
# print('RMSE train:', rmse(y_train, pred_train).round(3))
# print('RMSE valid:', rmse(y_valid, pred_valid).round(3))
# print('')

"""### **Cat Boost**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Cat Boost Model
# 
# cb = CatBoostRegressor(iterations=10, learning_rate=0.02, metric_period=100, loss_function='RMSE')
# cb.fit(X_train, y_train, eval_set=(X_valid, y_valid))
# 
# pred_train = cb.predict(X_train)
# pred_valid = cb.predict(X_valid)
# 
# print('')
# print('RMSE train:', rmse(y_train, pred_train).round(3))
# print('RMSE valid:', rmse(y_valid, pred_valid).round(3))
# print('')

"""**Insight**

Based on the several machine learning model that has been experimented on train dataset. The result obtained are listed as below:

1. Linear Regression Model
- RMSE train: 29.189
- RMSE valid: 40.539

2. Random Forest Regressor Model
- Max depth: None
- RMSE train: 7.792
- RMSE valid: 31.76

3. Light GBM Model
- RMSE train: 32.195
- RMSE valid: 52.586

4. Cat Boost Model
- RMSE train: 33.144
- RMSE valid: 54.083

## **Testing Model on Test Dataset**
"""

# Testing on test dataset

X_train_valid = train_valid.drop(['num_orders'], axis=1)
y_train_valid = train_valid['num_orders']

"""### **Linear Regression**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Linear Regression Model
# 
# lr = LinearRegression()
# lr.fit(X_train_valid, y_train_valid)
# 
# pred_train = lr.predict(X_train_valid)
# print('RMSE train:', rmse(y_train_valid, pred_train).round(3))
# 
# pred_test = lr.predict(X_test)
# print('RMSE test:', rmse(y_test, pred_test).round(3))
# 
# print('')

"""### **Random Forest**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Random Forest Regressor
# 
# rf = RandomForestRegressor(n_estimators=500)
# rf.fit(X_train_valid, y_train_valid)
# 
# pred_train = rf.predict(X_train_valid)
# print('RMSE train:', rmse(y_train_valid, pred_train).round(3))
# 
# pred_test = rf.predict(X_test)
# print('RMSE test:', rmse(y_test, pred_test).round(3))
# 
# print('')

"""### **Light GBM**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Light GBM Model
# 
# lg = LGBMRegressor(learning_rate=0.02, num_iterations=10, objective='rmse')
# lg.fit(X_train_valid, y_train_valid)
# 
# pred_train = lg.predict(X_train_valid)
# print('RMSE train:', rmse(y_train_valid, pred_train).round(3))
# 
# pred_test = lg.predict(X_test)
# print('RMSE test:', rmse(y_test, pred_test).round(3))
# 
# print('')

"""### **Cat Boost**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Cat Boost Model
# 
# cb = CatBoostRegressor(iterations=10, learning_rate=0.02, metric_period=100, loss_function='RMSE')
# cb.fit(X_train_valid, y_train_valid)
# 
# pred_train = cb.predict(X_train_valid)
# print('RMSE train:', rmse(y_train_valid, pred_train).round(3))
# 
# pred_test = cb.predict(X_test)
# print('RMSE test:', rmse(y_test, pred_test).round(3))
# 
# print('')

"""**Insight**

Based on the several machine learning model that has been experimented on test dataset. The result obtained are listed as below:

1. Linear Regression Model
- RMSE train: 30.489
- RMSE test: 53.178

2. Random Forest Regressor Model
- Max depth: None
- RMSE train: 8.239
- RMSE test: 46.198

3. Light GBM Model
- RMSE train: 34.461
- RMSE test: 77.177

4. Cat Boost Model
- RMSE train: 35.499
- RMSE test: 80.164

## **Conclusion**

The goal of this project is to formulate machine learning model to predict the order of the next hour that has the RMSE value less than 48.

Exploratory Data Analysis has been conducted and found that:
1. Data provided contains 26496 rows and 1 column of number of order for each 10 minutes of time interval.
2. There are no missing value has been found.
3. There are no duplicated value has been found.
4. The data shows consistency of increase order started from March to August 2018.

The development of machine learning model has been conducted with the 10% of dataset test and the results are as below:

Based on the several machine learning model that has been experimented on train dataset. The result obtained are listed as below:

1. Linear Regression Model
- RMSE train: 29.189
- RMSE valid: 40.539

2. Random Forest Regressor Model
- Max depth: None
- RMSE train: 7.792
- RMSE valid: 31.76

3. Light GBM Model
- RMSE train: 32.195
- RMSE valid: 52.586

4. Cat Boost Model
- RMSE train: 33.144
- RMSE valid: 54.083

Based on the several machine learning model that has been experimented on test dataset. The result obtained are listed as below:

1. Linear Regression Model
- RMSE train: 30.489
- RMSE test: 53.178

2. Random Forest Regressor Model
- Max depth: None
- RMSE train: 8.239
- RMSE test: 46.198

3. Light GBM Model
- RMSE train: 34.461
- RMSE test: 77.177

4. Cat Boost Model
- RMSE train: 35.499
- RMSE test: 80.164

Therefore, based on the machine learning performance on the test dataset, we can conclude that the best machine learning model that pass the specification of less than 48 on RMSE score is Randon Forest Regressor with n_estimators value of 500.
"""

